---
published: true
author: Charles
layout: post
title:  "Probabilistic Latent Semantic Analysis"
date:   2016-03-15 15:30
categories: 自然语言处理 推荐系统
---

之前谈过的 "[Latent Semantic Analysis][1]"，可以用来解决一义多词的问题，但缺乏严谨的数理统计基础，同时SVD分解非常耗时，pLSA就是在这样的背景下被提出的。

![此处输入图片的描述][2]


----------

#### pLSA
和很多模型一样，pLSA 遵从 bag-of-words 假设，即只考虑一篇文档中单词出现的次数，而忽略单词的先后次序关系，且每个单词的出现都是彼此独立的。

我们来看下pLSA的核心思想[^2]，

![此处输入图片的描述][3]

实心的节点 d 和 w 表示我们能观察到的文档和单词，空心节点 z 表示我们观察不到的隐藏变量，用来表示隐含的主题。

pLSA假设每篇文档的生成过程是这样的[^3]：

- 以 $P(d)$ 的先验概率选择一篇文档 $d$      
- 选定 d 后，以 $P(z\|d)$ 的概率选中主题 z       
- 选中主题 z 后，以 $P(w\|z)$ 的概率选中单词 w       

> 并且每个主题在所有词项上服从 Multinomial 分布，每个文档在所有主题上服从 Multinomial 分布。

![此处输入图片的描述][4]

同时我们认为在给定 Topic z 的条件下，单词 w 和文档 d 之间是条件独立的，也就说：

$$\begin{align*}
P(w|d,z) & = P(w|z)\\
P(w,d|z) & = P(w|z)P(d|z)
\end{align*}$$

那么，

$$\begin{align*}
P(w,d) &= P(d)P(w|d)  \tag{1}\\
P(w|d) & = \sum_{z\in \mathcal{Z}} P(w,z|d)\\
&= \sum_{z\in \mathcal{Z}} P(w|d)P(z|d) \tag{2}
\end{align*}$$

再来一发神图，

![此处输入图片的描述][5]


----------


####  Estimate parameters
模型中的参数包括： $P(z|d)$  和 $P(w|z)$。我们可以采用极大似然估计，目标函数为：

$$L = \prod_{(d,w)} P(w,d) = \prod_{d\in \mathcal{D}}\prod_{w\in \mathcal{W}} P(w,d)^{n(d,w)} \tag{3}$$

其中 $n(d,w)$ 表示单词 w 在文档 d 中出现的次数。

记 $\theta = (P(z\|d), P(w\|z))$ （我们要估计的参数）， 取对数，

$$\begin{align*}
\arg\max_\theta L(\theta)
& = \arg\max_\theta \sum_{d,w} n(d,w)\log P(d,w;\theta)\\
&=  \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta)P(d) \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta) + \underbrace{\sum_{d,w} n(d,w)\log P(d)}_{\text{与} \theta \text{无关项}} \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log \sum_z P(w|z)P(z|d)
\end{align*}$$

这是一个非凸优化（non-convex optimization）问题，我们利用EM算法来估计参数（对于这种包含**隐藏变量**的参数估计，果断选EM算法呀）。

----------

#### EM通俗版[^1]
我们先回味一下EM算法的核心思想。

> 简版：猜（E-step）, 反思（M-step）, 重复；

**啰嗦版**：你知道一些东西（观察的到的数据），你不知道一些东西（**隐藏变量**），你很好奇，想知道点那些不了解的东西。怎么办呢？     
你根据一些假设（parameter）**先猜**（E-step），把那些不知道的东西都猜出来，假装你全都知道了; 然后有了这些猜出来的数据，你**反思**一下，更新一下你的**假设**（parameter）,让你观察到的数据更加可能(Maximize likelihood; M-step); 然后再猜，再反思，最后，你就得到了一个可以解释整个数据的假设了。

$$\arg\max_\theta Q_t(\theta) = \arg\max_\theta \sum_{d,w} n(d,w) E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)] \tag{4}$$

----------


**E step:**

$$\begin{align*}
E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)]
&= \sum_z P(z|d,w;\theta_t) \log P(w, z|d;\theta) \\
&= \sum_z P(z|d,w;\theta_t) [\log P(w|z) + \log P(z|d)] \tag{5}
\end{align*}$$

在这一步，我们先猜出 $\theta = (P(z\|d), P(w\|z))$ ，然后我们就可以求出我们不知道的 $P(z\|d,w;\theta_t)$ :

$$P(z^{(k)}|d^{(i)},w^{(j)};\theta_t) = \frac{P_t(z^{(k)}|d^{(i)})P_t(w^{(j)}|z^{(k)})} {\sum_z P_t(z|d^{(i)})P_t(w^{(j)}|z)}$$

**M step:**

$$\begin{align*}
P_{t+1}(w^{(j)}|z^{(k)}) &= \frac {\sum_d n(d,w^{(j)})P(z^{(k)}|d,w^{(j)};\theta_t)} {\sum_{d,w} n(d,w)P(z^{(k)}|d,w;\theta_t)} \\
P_{t+1}(z^{(k)}|d^{(i)}) &= \frac {\sum_w n(d^{(i)},w)P(z^{(k)}|d^{(i)},w;\theta_t)} {\sum_{w,z} n(d,w)P(z|d^{(i)},w;\theta_t)}
\end{align*}$$

M step就是要让 $E_{z\|d,w;\theta_t}[\log P(w, z\|d;\theta)]$ 最大，利用**拉格朗日乘数法**（注意这个时候的**约束条件**：$\sum_w P(w\|z) = 1$ 和 $\sum_z P(z\|d) = 1$），就可以得到我们更新后的假设 $\theta^{t+1}$。

重复上述过程，直至收敛。

![此处输入图片的描述][6]

----------

####  Non-negative Matrix Factorization
另外，我们也可以通过矩阵分解的方式来呈现 pLSA，根据公式：

$$P(w,d) = \sum_{z\in \mathcal{Z}}P(z)P(d|z)P(w|z)$$

我们可以考虑利用矩阵分解的方式得到: $P(z),P(d\|z),P(w\|z)$. 不过这里用的是 Non-negative Matrix Factorization（非负矩阵分解），不是SVD呀.

> 利用矩阵分解来解决实际问题的分析方法很多，如PCA(主成分分析)、ICA(独立成分分析)、SVD(奇异值分解)、VQ(矢量量化)等。在所有这些方法中，原始的大矩阵 V 被近似分解为低秩的 $V=WH$ 形式。这些方法的共同特点是，因子 W 和 H 中的元素可为正或负，即使输入的初始矩阵元素是全正的，传统的秩削减算法也不能保证原始数据的**非负性**。在数学上，分解结果中存在负值是正确的，**但负值元素在实际问题中往往是没有意义的**。例如图像数据中不可能有负值的像素点；在文档统计中，负值也是无法解释的。

因为我们要得到的是概率，负值是没有意义的，所以这里采用的是NMF分解，具体大家还是去看看文献，琢磨下。

$$A = L\cdot U\cdot R$$

![此处输入图片的描述][7]

图中，矩阵 $\hat{A}$中， N 表示的是文档数， M 表示单词总数。

 - $L$ 包含文档概率 $P(d\|z)$.
 - $U$ 是主题的先验概率 P(z)，对角矩阵.
 - $R$ 表示单词概率 $P(w\|z)$.

----------

#### 总结
pLSA 的问题是在**文档层面**上没有一个概率模型，每篇文档的 $P(d\|z)$ 都是需要拟合的模型参数。 这就导致参数的数目会随文档数目线性增长、不能处理训练集外的文档这样的问题。

----------


  [1]: http://charlesx.top/2016/03/Latent-Semantic-Analysis/
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/f1_plsa.jpg
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_152408.png?imageView2/2/w/400
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/f2.jpg
  [5]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_161734.png?imageView2/2/w/400
  [6]: http://charlesx.top/assets/EM/its.png
  [7]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_214934.png
  
  [^1]: [Probabilistic latent semantic analysis (pLSA)](http://blog.tomtung.com/2011/10/plsa/)
  [^2]: [Probabilistic Latent Semantic Analysis](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf)
  [^3]: [概率语言模型及其变形系列(1)-PLSA及EM算法](http://blog.csdn.net/yangliuy/article/details/8330640)