---
published: true
author: Charles
layout: post
title:  "Probabilistic Latent Semantic Analysis"
date:   2016-03-15 15:30
categories: 自然语言处理 推荐系统
---

之前谈过的 "[Latent Semantic Analysis][1]"，可以用来解决一义多词的问题，但缺乏严谨的数理统计基础，同时SVD分解非常耗时，pLSA就是在这样的背景下被提出的[^3]。

![此处输入图片的描述][2]


----------

#### pLSA
和很多模型一样，pLSA 遵从 bag-of-words 假设，即只考虑一篇文档中单词出现的次数，而忽略单词的先后次序关系，且每个单词的出现都是彼此独立的。

我们来看下pLSA的核心思想[^2]，

![此处输入图片的描述][3]

实心的节点 d 和 w 表示我们能观察到的文档和单词，空心节点 z 表示我们观察不到的隐藏变量，用来表示隐含的主题。

pLSA假设每篇文档的生成过程是这样的：

- 以 $P(d)$ 的先验概率选择一篇文档 $d$      
- 选定 d 后，以 $P(z\|d)$ 的概率选中主题 z       
- 选中主题 z 后，以 $P(w\|z)$ 的概率选中单词 w       

> 并且每个主题在所有词项上服从 Multinomial 分布，每个文档在所有主题上服从 Multinomial 分布。

![此处输入图片的描述][4]

同时我们认为在给定 Topic z 的条件下，单词 w 和文档 d 之间是条件独立的，也就说：

$$\begin{align*}
P(w|d,z) & = P(w|z)\\
P(w,d|z) & = P(w|z)P(d|z)
\end{align*}$$

那么，

$$\begin{align*}
P(w,d) &= P(d)P(w|d)  \tag{1}\\
P(w|d) & = \sum_{z\in \mathcal{Z}} P(w,z|d)\\
&= \sum_{z\in \mathcal{Z}} P(w|d)P(z|d) \tag{2}
\end{align*}$$

再来一发神图，

![此处输入图片的描述][5]


----------


####  Estimate parameters
模型中的参数包括： $P(z|d)$  和 $P(w|z)$。我们可以采用极大似然估计，目标函数为：

$$L = \prod_{(d,w)} P(w,d) = \prod_{d\in \mathcal{D}}\prod_{w\in \mathcal{W}} P(w,d)^{n(d,w)} \tag{3}$$

其中 $n(d,w)$ 表示单词 w 在文档 d 中出现的次数。

记 $\theta = (P(z|d), P(w|z))$ （我们要估计的参数）， 取对数，

$$\begin{align*}
\arg\max_\theta L(\theta)
& = \arg\max_\theta \sum_{d,w} n(d,w)\log P(d,w;\theta)\\
&=  \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta)P(d) \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta) + \underbrace{\sum_{d,w} n(d,w)\log P(d)}_{\text{与} \theta \text{无关项}} \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log \sum_z P(w|z)P(z|d)
\end{align*}$$

这是一个非凸优化（non-convex optimization）问题，我们利用EM算法来估计参数（对于这种包含**隐藏变量**的参数估计，果断选EM算法呀）。

----------

#### EM通俗版[^1]
我们先回味一下EM算法的核心思想。

> 简版：猜（E-step）, 反思（M-step）, 重复；

**啰嗦版**：你知道一些东西（观察的到的数据），你不知道一些东西（**隐藏变量**），你很好奇，想知道点那些不了解的东西。怎么办呢？     
你根据一些假设（parameter）**先猜**（E-step），把那些不知道的东西都猜出来，假装你全都知道了; 然后有了这些猜出来的数据，你**反思**一下，更新一下你的**假设**（parameter）,让你观察到的数据更加可能(Maximize likelihood; M-step); 然后再猜，再反思，最后，你就得到了一个可以解释整个数据的假设了。

$$\arg\max_\theta Q_t(\theta) = \arg\max_\theta \sum_{d,w} n(d,w) E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)] \tag{4}$$

----------


**E step:**

$$\begin{align*}
E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)]
&= \sum_z P(z|d,w;\theta_t) \log P(w, z|d;\theta) \\
&= \sum_z P(z|d,w;\theta_t) [\log P(w|z) + \log P(z|d)] \\
\end{align*}$$

在这一步，我们先猜出 $\theta = (P(z\|d), P(w\|z))$ ，然后我们就可以求出我们不知道的 $P(z\|d,w;\theta_t)$ :

$$P(z^{(k)}|d^{(i)},w^{(j)};\theta_t) = \frac{P_t(z^{(k)}|d^{(i)})P_t(w^{(j)}|z^{(k)})} {\sum_z P_t(z|d^{(i)})P_t(w^{(j)}|z)}$$

**M step:**

$$\begin{align*}
P_{t+1}(w^{(j)}|z^{(k)}) &= \frac {\sum_d n(d,w^{(j)})P(z^{(k)}|d,w^{(j)};\theta_t)} {\sum_{d,w} n(d,w)P(z^{(k)}|d,w;\theta_t)} \\
P_{t+1}(z^{(k)}|d^{(i)}) &= \frac {\sum_w n(d^{(i)},w)P(z^{(k)}|d^{(i)},w;\theta_t)} {\sum_{w,z} n(d,w)P(z|d^{(i)},w;\theta_t)}
\end{align*}$$

M step就是要让 $E_{z\|d,w;\theta_t}[\log P(w, z\|d;\theta)]$ 最大，利用**拉格朗日乘数法**（注意这个时候的**约束条件**：$\sum_w P(w\|z) = 1$ 和 $\sum_z P(z\|d) = 1$），就可以得到我们更新后的假设 $\theta^{t+1}$。

重复上述过程，直至收敛。

![此处输入图片的描述][6]


----------


  [1]: http://charlesx.top/2016/03/Latent-Semantic-Analysis/
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/f1_plsa.jpg
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_152408.png?imageView2/2/w/400
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/f2.jpg
  [5]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_161734.png?imageView2/2/w/400
  [6]: http://charlesx.top/assets/EM/its.png
  
  [^1]: [Probabilistic latent semantic analysis (pLSA)](http://blog.tomtung.com/2011/10/plsa/)
  [^2]: [Probabilistic Latent Semantic Analysis](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf)
  [^3]: [概率语言模型及其变形系列(1)-PLSA及EM算法](http://blog.csdn.net/yangliuy/article/details/8330640)