---
published: true
author: Charles
layout: post
title:  "Probabilistic Latent Semantic Analysis"
date:   2016-03-15 15:30
categories: 自然语言处理 推荐系统
---

之前谈过的 "[Latent Semantic Analysis][1]"，可以用来解决一义多词的问题，但缺乏严谨的数理统计基础，同时SVD分解非常耗时，pLSA就是在这样的背景下被提出的。

![此处输入图片的描述][2]


----------

#### pLSA
和很多模型一样，pLSA 遵从 bag-of-words 假设，即只考虑一篇文档中单词出现的次数，而忽略单词的先后次序关系，且每个单词的出现都是彼此独立的。

我们来看下pLSA的核心思想[^2]（概率图模型），

![此处输入图片的描述][3]

实心的节点 d 和 w 表示我们能观察到的文档和单词，空心节点 z 表示我们观察不到的隐藏变量，用来表示隐含的主题。

pLSA假设每篇文档的生成过程是这样的[^3]：

- 以 $P(d)$ 的先验概率选择一篇文档 $d$      
- 选定 d 后，以 $P(z\|d)$ 的概率选中主题 z       
- 选中主题 z 后，以 $P(w\|z)$ 的概率选中单词 w       

> 并且每个主题在所有词项上服从 Multinomial 分布，每个文档在所有主题上服从 Multinomial 分布。


同时我们认为在给定 Topic z 的条件下，单词 w 和文档 d 之间是条件独立的，也就说：

$$\begin{align*}
P(w|d,z) & = P(w|z)\\
P(w,d|z) & = P(w|z)P(d|z)
\end{align*}$$

那么，

$$\begin{align*}
P(w,d) &= P(d)P(w|d)  \tag{1}\\
P(w|d) & = \sum_{z\in \mathcal{Z}} P(w,z|d)\\
&= \sum_{z\in \mathcal{Z}} P(w|d)P(z|d) \tag{2}
\end{align*}$$

再来一发神图，

![此处输入图片的描述][5]


----------


####  Estimate parameters
模型中的参数包括： $P(z|d)$  和 $P(w|z)$。我们可以采用极大似然估计，目标函数为：

$$L = \prod_{(d,w)} P(w,d) = \prod_{d\in \mathcal{D}}\prod_{w\in \mathcal{W}} P(w,d)^{n(d,w)} \tag{3}$$

其中 $n(d,w)$ 表示单词 w 在文档 d 中出现的次数。

记 $\theta = (P(z\|d), P(w\|z))$ （我们要估计的参数）， 取对数，

$$\begin{align*}
\arg\max_\theta L(\theta)
& = \arg\max_\theta \sum_{d,w} n(d,w)\log P(d,w;\theta)\\
&=  \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta)P(d) \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log P(w|d;\theta) + \underbrace{\sum_{d,w} n(d,w)\log P(d)}_{\text{与} \theta \text{无关项}} \\
&= \arg\max_\theta \sum_{d,w} n(d,w)\log \sum_z P(w|z)P(z|d)
\end{align*}$$

这是一个非凸优化（non-convex optimization）问题，我们利用EM算法来估计参数（对于这种包含**隐藏变量**的参数估计，果断选EM算法呀）。

----------

#### EM通俗版[^1]
我们先回味一下EM算法的核心思想。

> 简版：猜（E-step）, 反思（M-step）, 重复；

**啰嗦版**：你知道一些东西（观察的到的数据），同时又不知道一些东西（**隐藏变量**），你很好奇，想知道点那些不了解的东西。怎么办呢？     
你根据一些假设（parameter）**先猜**（E-step），把那些不知道的东西都猜出来，假装你全都知道了; 然后有了这些猜出来的数据，你**反思**一下，更新一下你的**假设**（parameter）,让你观察到的数据更加可能(Maximize likelihood; M-step); 然后再猜，再反思，最后，你就得到了一个可以解释整个数据的假设了。

$$\arg\max_\theta Q_t(\theta) = \arg\max_\theta \sum_{d,w} n(d,w) E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)] \tag{4}$$

----------


**E step:**

$$\begin{align*}
E_{z|d,w;\theta_t}[\log P(w, z|d;\theta)]
&= \sum_z P(z|d,w;\theta_t) \log P(w, z|d;\theta) \\
&= \sum_z P(z|d,w;\theta_t) [\log P(w|z) + \log P(z|d)] \tag{5}
\end{align*}$$

在这一步，我们先猜出 $\theta = (P(z\|d), P(w\|z))$ ，然后我们就可以求出我们不知道的 $P(z\|d,w;\theta_t)$ :

$$P(z^{(k)}|d^{(i)},w^{(j)};\theta_t) = \frac{P_t(z^{(k)}|d^{(i)})P_t(w^{(j)}|z^{(k)})} {\sum_z P_t(z|d^{(i)})P_t(w^{(j)}|z)}$$

**M step:**

$$\begin{align*}
P_{t+1}(w^{(j)}|z^{(k)}) &= \frac {\sum_d n(d,w^{(j)})P(z^{(k)}|d,w^{(j)};\theta_t)} {\sum_{d,w} n(d,w)P(z^{(k)}|d,w;\theta_t)} \\
P_{t+1}(z^{(k)}|d^{(i)}) &= \frac {\sum_w n(d^{(i)},w)P(z^{(k)}|d^{(i)},w;\theta_t)} {\sum_{w,z} n(d,w)P(z|d^{(i)},w;\theta_t)}
\end{align*}$$

M step就是要让 $E_{z\|d,w;\theta_t}[\log P(w, z\|d;\theta)]$ 最大，利用**拉格朗日乘数法**（注意这个时候的**约束条件**：$\sum_w P(w\|z) = 1$ 和 $\sum_z P(z\|d) = 1$），就可以得到我们更新后的假设 $\theta^{t+1}$。

重复上述过程，直至收敛。

![此处输入图片的描述][6]

----------

####  Non-negative Matrix Factorization
另外，我们也可以通过矩阵分解的方式来呈现 pLSA，根据公式：

$$P(w,d) = \sum_{z\in \mathcal{Z}}P(z)P(d|z)P(w|z)$$

我们可以考虑利用矩阵分解的方式得到: $P(z),P(d\|z),P(w\|z)$. 不过这里用的是 Non-negative Matrix Factorization（非负矩阵分解），不是SVD呀.

> 利用矩阵分解来解决实际问题的分析方法很多，如PCA(主成分分析)、ICA(独立成分分析)、SVD(奇异值分解)、VQ(矢量量化)等。在所有这些方法中，原始的大矩阵 V 被近似分解为低秩的 $V=WH$ 形式。这些方法的共同特点是，因子 W 和 H 中的元素可为正或负，即使输入的初始矩阵元素是全正的，传统的秩削减算法也不能保证原始数据的**非负性**。在数学上，分解结果中存在负值是正确的，**但负值元素在实际问题中往往是没有意义的**。例如图像数据中不可能有负值的像素点；在文档统计中，负值也是无法解释的。

因为我们要得到的是概率，负值是没有意义的，所以这里采用的是NMF分解，具体大家还是去看看文献，琢磨下。

$$A = L\cdot U\cdot R \tag{6}$$

![此处输入图片的描述][7]

图中，矩阵 $\hat{A}$中， N 表示的是文档数， M 表示单词总数。

 - $L$ 包含文档概率 $P(d\|z)$.
 - $U$ 是主题的先验概率 P(z)，对角矩阵.
 - $R$ 表示单词概率 $P(w\|z)$.

----------

#### LSA vs pLSA[^4]

![此处输入图片的描述][8]

从图中我们可以看到pLSA与LSA之间的对应关系。其中 $p(z)$ 刻画了 Latent Space 也即 topic space 的信息； $p(w\|z)$ 刻画了 topic space 与 term space 之间的关系，对应着 LSA 中的正交基 V ；在文档分类时，这两部分也就是我们在模型训练结束需要保存的信息，当一个新的文档的到来时， 我们可以再次利用 EM 算法得到新的文档与主题的对应关系 $p(d\|z)$ ，并由此得到文档在 topic 空间上的表示 $p(z\|d)$。

----------

#### 总结
**pLSA的优势：**

 - 定义了概率模型，而且每个变量以及相应的概率分布和条件概率分布都有明确的物理解释；
 - 相比于 LSA 隐含的高斯分布假设，pLSA 隐含的 Multi-nomial 分布假设更符合文本特性；
 - pLSA 的优化目标是是 KL-divergence 最小，而不是依赖于最小均方误差等准则；
 - 可以利用各种 model selection 和 complexity control 准则来确定 topic 的维数；
 

**pLSA的不足：**

 - 概率模型不够完备：在 document 层面上没有提供合适的概率模型，使得pLSA并不是完备的生成式模型
 - 随着 document 和 term 个数的增加，pLSA模型复杂度也线性增加，变得越来越庞大；


针对 pLSA 的不足，研究者们又提出了各种各样的 topic based model , 其中包括大名鼎鼎的 Latent Dirichlet Allocation (LDA，生成式模型)。

----------


  [^1]: [Probabilistic latent semantic analysis (pLSA)](http://blog.tomtung.com/2011/10/plsa/)
  [^2]: [Probabilistic Latent Semantic Analysis](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf)
  [^3]: [概率语言模型及其变形系列(1)-PLSA及EM算法](http://blog.csdn.net/yangliuy/article/details/8330640)
  [^4]: [Probabilistic Latent Semantic Analysis](https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf)



  [1]: http://charlesx.top/2016/03/Latent-Semantic-Analysis/
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/f1_plsa.jpg
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_152408.png?imageView2/2/w/300
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/f2.jpg
  [5]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_161734.png?imageView2/2/w/300
  [6]: http://charlesx.top/assets/EM/its.png
  [7]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_214934.png
  [8]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-17_093803.png
  