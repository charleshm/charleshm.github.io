---
published: true
author: Charles
layout: post
title:  "Probabilistic Latent Semantic Analysis"
date:   2016-03-15 15:30
categories: 自然语言处理 推荐系统
---

之前谈过的 "[Latent Semantic Analysis][1]"，可以用来解决一义多词的问题，但缺乏严谨的数理统计基础，同时SVD分解非常耗时，pLSA就是在这样的背景下被提出的。

![此处输入图片的描述][2]


----------

#### pLSA
和很多模型一样，pLSA 遵从 bag-of-words 假设，即只考虑一篇文档中单词出现的次数，而忽略单词的先后次序关系，且每个单词的出现都是彼此独立的。

我们来看下pLSA的核心思想，

![此处输入图片的描述][3]

实心的节点 d 和 w 表示我们能观察到的文档和单词，空心节点 z 表示我们观察不到的隐藏变量，用来表示隐含的主题。

pLSA假设每篇文档的生成过程是这样的：

- 以 $P(d)$ 的先验概率选择一篇文档 $d$      
- 选定 d 后，以 $P(z\|d)$ 的概率选中主题 z       
- 选中主题 z 后，以 $P(w\|z)$ 的概率选中单词 w       

> 并且每个主题在所有词项上服从 Multinomial 分布，每个文档在所有主题上服从 Multinomial 分布。

同时我们认为在给定 Topic z 的条件下，单词 w 和文档 d 之间是条件独立的，也就说：

$$P(w|d,z) = P(w|z)$$

那么，

$$\begin{align*}
P(w,d) &= P(d)P(w|d) \\
P(w|d) & = \sum_{z\in \mathcal{Z}} P(w,z|d)\\
&= \sum_{z\in \mathcal{Z}} P(w|d,z)P(z|d)
\end{align*}$$

即，

$$\begin{align*}
P(w|d) &= \sum_{z\in \mathcal{Z}} P(w|z)P(z|d) \tag{1}\\
P(w,d) &= \sum_{z\in \mathcal{Z}} P(z)P(d|z)P(w|z) \tag{2}
\end{align*}$$

再来一发神图，

![此处输入图片的描述][4]


----------


####  Estimate parameters
模型中的参数包括： $P(z|d)$  和 P(w|z)。我们可以采用极大似然估计，目标函数为：

$$L = \prod_{(d,w)} P(w|d) = \prod_{d\in \mathcal{D}}\prod_{w\in \mathcal{W}} P(w|d)^{n(d,w)} \tag{3}$$

其中 $n(d,w)$ 表示单词 w 在文档 d 中出现的次数。

取对数，

$$\mathcal{L} =\log L = \sum_{d\in \mathcal{D}}\sum_{w\in \mathcal{W}} n(d,w)\log \sum_{z \in \mathcal{Z}} P(w|z)P(z|d)) \tag{4}$$

这是一个非凸优化（non-convex optimization）问题，我们利用EM算法来估计参数（对于这种包含**隐藏变量**的参数估计，果断选EM算法呀）。

----------


  [1]: http://charlesx.top/2016/03/Latent-Semantic-Analysis/
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/f1_plsa.jpg
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_152408.png?imageView2/2/w/400
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-16_161734.png?imageView2/2/w/400