---
published: true
author: Charles
layout: post
title:  "PCA 与 SVD"
date:   2016-03-13 19:35
categories: 机器学习
---

PCA（Principal Component Analysis）是一种常用的数据降维方法，通过线性变换将原始数据变换为一组各维度**线性无关**的表示，以提取数据的主要特征成分。本文着重于理清PCA的数学原理，并给出相应的证明[^1]。


----------


#### 数据中的冗余和噪声
我们先举一个例子，假设现在我们拿到这样一组数据，里面有两个属性，既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然我们一眼就看出这两个特征有一个多余。

![此处输入图片的描述][1]

怎么直观的判断数据是否冗余? 上图从左往右，我们可以发现数据之间的关联性越来越强，也就是说两组数据越来越“相似”，我们用其中一组数据就能预测出另一组数据，这就是数据需要降维的其中一个原因：**冗余**。

另一方面，除了冗余之外，数据中可能还存在噪声，数据记录过程中存在某些不可抗因素的干扰。我们常常用信噪比signal-to-noise ratio (SNR) 来评价一组数据的好坏。

$$SNR = \frac{\delta_{signal}^2}{\delta_{noise}^2} \tag{1}$$

$SNR \gg 1$意味着数据比较纯净，可信度比较高，SNR 较小时，我们常说信号被噪声淹没了。

![此处输入图片的描述][2]

> 换句话说，一组数据中” 信号” 部分的**方差**较大，方差较小的我们可以认定为噪声。

哎呀，我们已经找到PCA 的理论根据了，我们要得到数据中的” 有用” 信息，也就是说我们要找到使数据方差最大的方向。

我们也知道了，PCA 的两大作用，**降低冗余，减小噪声**。


----------


#### 协方差矩阵
那么怎么判断两组数据是否冗余呢，很简单呀，求个协方差就可以了。

举个例子，我们想知道一个男孩子的猥琐程度跟他受女孩子的欢迎程度是否存在一些联系。我们就可以用协方差来度量，协方差的的定义：

$$\delta_{ab} = \frac{1}{n-1}ab^T$$

其中，$a = [a_1,a_2,\cdots,a_n]$，$b = [b_1,b_2,\cdots,b_n]$ . 

协方差的结果有什么意义呢？

如果结果为正值，则说明两者是正相关的（从协方差可以引出“相关系数”的定义），也就是说一个人越猥琐越受女孩欢迎。如果结果为负值，就说明两者是负相关，越猥琐女孩子越讨厌。如果为0，则两者之间没有关系，猥琐不猥琐和女孩子喜不喜欢之间没有关联，就是统计上说的“相互独立”。

下面我们来说一说协方差矩阵，假设我们现在有一个 $m \times n$ 的矩阵 $X$ ，

$$X = \left(  
\begin{matrix}
     \mathbf{x}_1  \\
     \mathbf{x}_2  \\
     \vdots\\
     \mathbf{x}_m
\end{matrix} 
\right)$$

协方差矩阵可表示为:

$$S_X = \frac{1}{n-1} XX^T \tag{2}$$

我们要消除冗余，也就是要使$S_X$非对角线上的元素为0，也就是使得矩阵 $X$ 的任意两行**线性无关**。

----------

#### 数学描述

我们得到了降维问题可以表达为这样一个过程：

> 将一组 N 维向量降为K 维（K 大于0，小于N），其目标是选择K 个单位（模为1）**正交基**，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K 个方差）。

**这样我们就即减小了冗余，又降低了噪声！**


----------


#### 对角化协方差矩阵
假设我们现在选取的这组基为$P$:

$$P = \left(  
\begin{matrix}
     \mathbf{p}_1  \\
     \mathbf{p}_2  \\
     \vdots\\
     \mathbf{p}_m
\end{matrix} 
\right)$$

$X$在这组新基下的坐标为：

$$Y = PX $$

协方差矩阵：

$$\begin{align*}
S_Y & = \frac{1}{n-1} YY^T\\
& = \frac{1}{n-1} (PX)(PX)^T\\
& = \frac{1}{n-1} P(XX^T)P^T
\end{align*}$$


令,

$$A = XX^T$$

也就是说，我们希望找到一组基来对角化 $A$，注意到 A 是对称矩阵，学过线性代数的都知道，对阵矩阵是肯定可以对角化的。

> 一个n 行n 列的实对称矩阵 A 一定可以找到 n 个单位正交特征向量 $E = (e_1,e_2,\cdots,e_n )$，使得：

$$E^TAE = 
\left( \begin{array}{ccccc}
    \lambda_1 &  &  &   &   \\
      & \lambda_2 &   &   &   \\
      &   & \ddots &  &   \\
      &   &   & \lambda_n &  
\end{array} \right) \tag{3}$$

----------

#### SVD 和 PCA
PCA 实际上就是对角化 $XX^T$，还记得我们前面推导 SVD 的过程吗，我们是从特征值分解开始的，

$$\begin{align*}
A^TA & = VDV^T\\
\Rightarrow A &= U\Sigma V^T
\end{align*}$$



----------


  [1]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-14_164210.png?imageView2/2/w/500
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/2016-03-14_164741.png?imageView2/2/w/300

  [^1]: [A TUTORIAL ON PRINCIPAL COMPONENT ANALYSIS](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf)



