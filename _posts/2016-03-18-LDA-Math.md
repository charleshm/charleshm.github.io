---
published: true
author: Charles
layout: post
title:  "LDA中的数学知识"
date:   2016-03-18 15:30
categories: 机器学习 自然语言处理
---

之后准备花时间看看 Bayesian Statistics，现在的了解不够系统。

#### 频率学派 vs 贝叶斯学派
> 频率派把需要推断的参数 $\theta$ 看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本 X 是随机的，所以频率派重点研究 **样本空间**，大部分的概率计算都是针对样本 X 的分布；而贝叶斯派的观点则截然相反，他们认为待估计的参数是 **随机变量**，服从一定的分布，而样本 X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布[^6]。

$$\text{先验信息}+\text{样本信息} \Rightarrow \text{后验分布}$$

----------

#### 贝叶斯定理
 现在假设我们有这样几类概率：$P(\theta)$（先验分布）, $P(\theta\|x)$（后验分布）, $P(x)$, $P(x\|\theta)$ （似然函数） .

我们知道，很多时候，光靠 $P(x\|\theta)$ （即“似然”）是不够的，有时候还需要引入 $P(\theta)$ 这个先验概率。奥卡姆剃刀指的是 $P(\theta)$ 较大的模型有较大的优势，而最大似然则是说最符合观测数据的（即 $P(x\|\theta)$ 最大的）最有优势。

$$P(\theta|x) = \frac{P(x|\theta)P(\theta)}{\int P(x|\theta)P(\theta)d\theta} \propto P(x|\theta)P(\theta) \tag{1}$$

> 我们不妨再举一个简单的例子来说明这一精神：你随便找枚硬币，掷一下，观察一下结果。好，你观察到的结果要么是“正”，要么是“反”，不妨假设你观察到的是“正”。现在你要去根据这个观测数据推断这枚硬币掷出“正”的概率是多大。根据最大似然估计的精神，我们应该猜测这枚硬币掷出“正”的概率是 1 ，因为这个才是能最大化 $P(x\|\theta)$ 的那个猜测。这样很显然是不切实际的，我们对一枚随机硬币是否为一枚有偏硬币，偏了多少，是有着一个**先验的认识**的，这个认识就是绝大多数硬币都是基本公平的，偏得越多的硬币越少见（可以用一个 **beta 分布** 来表达这一先验概率）。将这个先验正态分布 $p(\theta)$ （其中 $\theta$ 表示硬币掷出正面的比例，小写的 p 代表这是**概率密度函数**）结合到我们的问题中，我们便不是去最大化 $P(x\|\theta)$ ，而是去最大化 $P(x\|\theta)P(\theta)$ 。显然 $\theta = 1$ 是不行的，因为 $P(\theta=1)$ 为 0，导致整个乘积也为 0 。实际上，只要对这个式子求一个导数就可以得到最值点。[^1]

![此处输入图片的描述][1]

----------


#### 先验分布
通过上面的分析，我们知道光用最大似然是不靠谱的，因为最大似然的猜测可能先验概率非常小。

$$\mathrm{posterior} \propto \mathrm{prior} \times \mathrm{likelihood} \tag{2}$$


常用的先验分布的形式[^4]，

1. **无信息先验**（uninformative Priors） 无信息先验只包含了参数的模糊的或者一般的信息，是对后验分布**影响最小**的先验分布。很多人愿意选取无信息先验，因为这种先验与其它“主观”的先验相比更接近“客观”。通常，我们把均匀分布作为无信息先验来使用，这相当于在参数所有的可能值上边指派了相同的似然。但是无先验信息的使用也要慎重，比如有些情况下会导致不恰当的后验分布（如不可积分的后验概率密度）。

2. **Jeffreys先验**(Jeffreys’ Prior：杰佛里先验分布) Jeffreys提出的选取先验分布的原则是一种不变原理，采用 Fisher 信息阵的平方根作为 $\theta$ 的无信息先验分布。较好地解决了无信息先验中的一个矛盾，即若对参数θ选用均匀分布，则其函数 $g(\theta)$ 往往不是均匀分布。

3. **信息先验**（Informative Priors） 根据以前的经验、研究或专家经验得到的先验分布。

4. **共轭先验**（Conjugate Priors） 共轭先验是指先验分布和后验分布来自同一个分布族的情况，就是说先验和后验有相同的分布形式（当然，参数是不同的）。**这些共轭先验是结合似然的形式推导出来的**。共轭先验是经常被使用的一种先验分布形式，原因在于数学处理和**计算上**的方便性，同时后验分布的一些参数也可以有很好的**解释**。

那么，我们如何选取先验概率分布呢？

**先验分布的选取应以合理性为首要原则,**

> 如果是离散的情况，根据经验或者专家意见形成主观概率就可以得到先验分布。在信息充分的情况下，利用分参数密度估计（如直方图）寻找先验分布，判断似然分布的形式选择**共轭先验分布**也是一种比较方便的方法。如果没有先验信息，或者先验信息很模糊的情况下，选择无信息先验分布，也可以根据似然函数的形式选择**共轭先验分布**。

----------

#### 共轭先验分布
由于共轭先验在计算上的优势，同时具备良好的可解释性，因此在实际中应用很广，这个概念最先是由 Raiffa and Schlaifer 在1961年提出。

> We shall see that an import role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior , and that therefore lead to a greatly simplified Bayesian analysis.（摘自 "PRML"）

![此处输入图片的描述][2]

> 在贝叶斯概率理论中，如果后验概率 $P(\theta\|x)$ 和先验概率 $p(\theta)$ 满足同样的分布，那么，先验分布和后验分布被叫做共轭分布，同时，**先验分布**称为**似然函数**的共轭先验分布。

**注意，共轭指的是先验分布和似然函数共轭。**


----------


#### 举个栗子[^5]
假设我们现在有一位选手在5轮射击比赛中的数据，每轮25次射击，我们需要评判下他的射击准度，

<p align="center">Round 1: 13/25，Round 2: 12/25，Round 3: 14/25，Round 4: 19/25，Total: 58/100</p>

我们先看下似然函数( $\theta$ 表示射中的概率)，

$$P(x|\theta) = \theta^x(1-\theta)^{n-x} \tag{3}$$

![此处输入图片的描述][3]

二项分布的共轭先验为 beta 分布，它有两个超参数（hyperparameters），决定了 $\theta$ 的分布（分布的分布）。

$$P(\theta|\alpha,\beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\int_{0}^{1} \theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta} \tag{4}$$

$\alpha,\beta$ 的值，可以根据我们事先对该选手的了解来选择，假设我们现在选择了下面的三种先验分布，我们想分析下先验分布对后验分布的影响。

![此处输入图片的描述][4]

后验分布可表示为，

$$\begin{align*}
P(\theta|x) & \propto P(x|\theta)P(\theta)\\
& \propto \theta^x(1-\theta)^{n-x}\theta^{\alpha-1}(1-\theta)^{\beta-1}\\
& = \theta^{(x+\alpha)-1}(1-\theta)^{(\beta+n-x)-1}\\
& \rightarrow Beta(\alpha+x,\beta+n-x) \tag{5}
\end{align*}$$

> 用上了共轭先验，计算起来 so easy， 形式统一，可解释性强 .

我们利用增量学习的方式来更新参数，前一轮得到的后验分布将作为下一轮的先验分布。

Round 1: 13/25，

![此处输入图片的描述][5]

Round 2: 12/25，

![此处输入图片的描述][6]

Round 3: 14/25，

![此处输入图片的描述][7]

Round 4: 19/25，

![此处输入图片的描述][8]

我们总结一下，先验概率对我们后验概率的影响[^3]，


----------


- If the prior is uninformative, the posterior is very much determined by the data (the posterior is data-driven).
- If the prior is informative, the posterior is a mixture of the prior and the data.The more informative the prior, the more data you need to "change" your beliefs, so to speak because the posterior is very much driven by the prior information.
- If you have a lot of data, the data will dominate the posterior distribution (they will overwhelm the prior).

----------


  [^1]: [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)
  [^2]: [共轭分布与共轭先验](http://blog.huanghao.me/?p=250)
  [^3]: [Help me understand Bayesian prior and posterior distributions](http://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions)
  [^4]: [贝叶斯统计基础](https://site.douban.com/182577/widget/notes/10567181/note/294041203/?)
  [^5]: [Understanding Bayes: Updating priors via the likelihood](http://alexanderetz.com/2015/07/25/understanding-bayes-updating-priors-via-the-likelihood/)
  [^6]: [通俗理解LDA主题模型](http://blog.csdn.net/v_july_v/article/details/41209515)


  [1]: http://7xjbdi.com1.z0.glb.clouddn.com/Beta_distribution_pdf.svg.png
  [2]: http://7xjbdi.com1.z0.glb.clouddn.com/0000041497.gif
  [3]: http://7xjbdi.com1.z0.glb.clouddn.com/fig1likelihood.png?imageView2/2/w/200
  [4]: http://7xjbdi.com1.z0.glb.clouddn.com/three_prior.png
  [5]: http://7xjbdi.com1.z0.glb.clouddn.com/first_round.png
  [6]: http://7xjbdi.com1.z0.glb.clouddn.com/round_2.png
  [7]: http://7xjbdi.com1.z0.glb.clouddn.com/round_3.png
  [8]: http://7xjbdi.com1.z0.glb.clouddn.com/round4.png