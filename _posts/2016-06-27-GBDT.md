---
published: true
author: Charles
layout: post
title:  "Gradient Boosting模型"
date:   2016-06-27 8:30
categories: 机器学习
---

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，每个新模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

总的来说，Gradient Boosting是一种结合函数梯度下降和Boosting思想的算法。

$$\text{Gradient Boosting} = \text{Gradient Descent} + \text{Boosting}$$

---

#### 算法流程

[Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

---

#### XGBoost

GBDT+LR:

[](http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf)

推荐系统：
http://km.oa.com/group/19658/articles/show/209927?kmref=search&from_page=1&no=4&is_from_iso=1

---

#### XGBoost vs GBDT

> 以下内容主要摘自 [wepon](http://2hwp.com/) 博客。

---

- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

![][1]

---

- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

![][2]

---

- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。


XGBoost的特征重要性是如何得到的？

> 某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和，比如特征A在第一次迭代中（即第一棵树）被选中了1次去分裂树节点，在第二次迭代被选中2次…..那么最终特征A的feature score就是 1+2+….

---

#### 新进展

[Support Dropout on XGBoost](http://dmlc.ml/xgboost/2016/07/02/support-dropout-on-xgboost.html)

[1]:http://7xjbdi.com1.z0.glb.clouddn.com/xgb%2BTaylor.png
[2]:http://7xjbdi.com1.z0.glb.clouddn.com/xgb_c_t.png


[^1]:[决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)
[^2]:[深入浅出ML之Boosting家族](http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/)
[^3]:[A Gentle Introduction to Gradient Boosting](http://www.chengli.io/tutorials/gradient_boosting.pdf)
[^4]:[Greedy Function Approximation: A Gradient Boosting Machine](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

